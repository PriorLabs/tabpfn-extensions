"""
Utility functions for TabPFGen data synthesizer extension.
"""

import numpy as np
import pandas as pd
from typing import Tuple, Union, Optional, Dict, Any
import warnings


def validate_tabpfn_data(
    X: Union[np.ndarray, pd.DataFrame],
    y: Union[np.ndarray, pd.Series],
    max_samples: int = 10000,
    max_features: int = 100
) -> Tuple[bool, str]:
    """
    Validate data compatibility with TabPFN requirements.
    
    Parameters
    ----------
    X : array-like
        Feature data
    y : array-like  
        Target data
    max_samples : int, default=10000
        Maximum number of samples recommended for TabPFN
    max_features : int, default=100
        Maximum number of features recommended for TabPFN
        
    Returns
    -------
    is_valid : bool
        Whether data meets TabPFN requirements
    message : str
        Validation message or warning
    """
    X = np.asarray(X)
    y = np.asarray(y)
    
    # Handle edge cases
    if X.size == 0 or y.size == 0:
        return False, "Dataset is empty"
    
    if len(X) != len(y):
        return False, "X and y have mismatched dimensions"
    
    n_samples, n_features = X.shape
    
    warnings_list = []
    
    # Check sample count
    if n_samples > max_samples:
        warnings_list.append(
            f"Dataset has {n_samples} samples, but TabPFN works best with <={max_samples} samples"
        )
    
    # Check feature count
    if n_features > max_features:
        warnings_list.append(
            f"Dataset has {n_features} features, but TabPFN works best with <={max_features} features"
        )
    
    # Check for missing values
    if np.isnan(X).any():
        warnings_list.append("Dataset contains missing values - consider imputation")
    
    # Check for infinite values
    if np.isinf(X).any():
        warnings_list.append("Dataset contains infinite values")
    
    # For classification, check class distribution
    unique_classes = np.unique(y)
    if len(unique_classes) <= 20 and len(unique_classes) > 0:  # Assume classification if few unique values
        unique_classes, counts = np.unique(y, return_counts=True)
        
        # Only check if we have actual counts
        if len(counts) > 0:
            min_count = np.min(counts)
            if min_count < 2:
                warnings_list.append("Some classes have very few samples - consider class balancing")
            
            # Check for imbalance
            max_count = np.max(counts)
            imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
            if imbalance_ratio > 5:
                warnings_list.append(f"Dataset is imbalanced (ratio: {imbalance_ratio:.1f}:1) - consider using balance_dataset()")
    
    is_valid = len(warnings_list) == 0
    message = "; ".join(warnings_list) if warnings_list else "Data validation passed"
    
    return is_valid, message


def combine_datasets(
    X_original: Union[np.ndarray, pd.DataFrame],
    y_original: Union[np.ndarray, pd.Series],
    X_synthetic: np.ndarray,
    y_synthetic: np.ndarray,
    strategy: str = 'append'
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Combine original and synthetic datasets.
    
    Parameters
    ----------
    X_original, y_original : array-like
        Original training data
    X_synthetic, y_synthetic : array-like
        Synthetic data generated by TabPFGen
    strategy : str, default='append'
        How to combine data: 'append', 'replace', or 'balanced'
        
    Returns
    -------
    X_combined : ndarray
        Combined feature data
    y_combined : ndarray
        Combined target data
    """
    X_orig = np.asarray(X_original)
    y_orig = np.asarray(y_original)
    
    if strategy == 'append':
        # Simply append synthetic data to original
        X_combined = np.vstack([X_orig, X_synthetic])
        y_combined = np.hstack([y_orig, y_synthetic])
    
    elif strategy == 'replace':
        # Use only synthetic data
        X_combined = X_synthetic
        y_combined = y_synthetic
    
    elif strategy == 'balanced':
        # Balance original and synthetic data equally
        n_orig = len(X_orig)
        n_synth = len(X_synthetic)
        
        if n_synth > n_orig:
            # Subsample synthetic data
            indices = np.random.choice(n_synth, n_orig, replace=False)
            X_synthetic = X_synthetic[indices]
            y_synthetic = y_synthetic[indices]
        elif n_synth < n_orig:
            # Subsample original data
            indices = np.random.choice(n_orig, n_synth, replace=False)
            X_orig = X_orig[indices]
            y_orig = y_orig[indices]
        
        X_combined = np.vstack([X_orig, X_synthetic])
        y_combined = np.hstack([y_orig, y_synthetic])
    
    else:
        raise ValueError("strategy must be 'append', 'replace', or 'balanced'")
    
    return X_combined, y_combined


def analyze_class_distribution(
    y: Union[np.ndarray, pd.Series],
    title: str = "Class Distribution"
) -> Dict[str, Any]:
    """
    Analyze and display class distribution statistics.
    
    Parameters
    ----------
    y : array-like
        Target labels
    title : str
        Title for the analysis
        
    Returns
    -------
    analysis : dict
        Dictionary containing distribution statistics
    """
    y = np.asarray(y)
    unique_classes, counts = np.unique(y, return_counts=True)
    
    total_samples = len(y)
    percentages = counts / total_samples * 100
    
    # Calculate imbalance metrics
    max_count = np.max(counts)
    min_count = np.min(counts)
    imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
    
    analysis = {
        'title': title,
        'classes': unique_classes.tolist(),
        'counts': counts.tolist(),
        'percentages': percentages.tolist(),
        'total_samples': total_samples,
        'num_classes': len(unique_classes),
        'max_count': max_count,
        'min_count': min_count,
        'imbalance_ratio': imbalance_ratio,
        'is_balanced': imbalance_ratio <= 2.0  # Reasonable threshold
    }
    
    print(f"\n=== {title} ===")
    for cls, count, pct in zip(unique_classes, counts, percentages):
        print(f"Class {cls}: {count} samples ({pct:.1f}%)")
    
    print(f"Total: {total_samples} samples, {len(unique_classes)} classes")
    print(f"Imbalance ratio: {imbalance_ratio:.1f}:1")
    
    return analysis


def calculate_synthetic_quality_metrics(
    X_original: Union[np.ndarray, pd.DataFrame],
    X_synthetic: np.ndarray,
    y_original: Optional[Union[np.ndarray, pd.Series]] = None,
    y_synthetic: Optional[np.ndarray] = None
) -> Dict[str, float]:
    """
    Calculate quality metrics comparing original and synthetic data.
    
    Parameters
    ----------
    X_original : array-like
        Original feature data
    X_synthetic : array-like
        Synthetic feature data
    y_original, y_synthetic : array-like, optional
        Original and synthetic labels
        
    Returns
    -------
    metrics : dict
        Dictionary of quality metrics
    """
    X_orig = np.asarray(X_original)
    X_synth = np.asarray(X_synthetic)
    
    metrics = {}
    
    # Feature distribution comparison
    try:
        mean_orig = np.mean(X_orig, axis=0)
        mean_synth = np.mean(X_synth, axis=0)
        metrics['mean_absolute_error'] = np.mean(np.abs(mean_orig - mean_synth))
        
        std_orig = np.std(X_orig, axis=0)
        std_synth = np.std(X_synth, axis=0)
        metrics['std_absolute_error'] = np.mean(np.abs(std_orig - std_synth))
        
    except Exception as e:
        warnings.warn(f"Could not calculate distribution metrics: {e}")
    
    # Correlation comparison
    try:
        if X_orig.shape[1] > 1:  # Need at least 2 features for correlation
            corr_orig = np.corrcoef(X_orig.T)
            corr_synth = np.corrcoef(X_synth.T)
            
            # Compare upper triangle (excluding diagonal)
            mask = np.triu(np.ones_like(corr_orig), k=1).astype(bool)
            if np.sum(mask) > 0:  # Ensure we have correlations to compare
                corr_diff = np.abs(corr_orig[mask] - corr_synth[mask])
                metrics['correlation_mae'] = np.mean(corr_diff)
                
    except Exception as e:
        warnings.warn(f"Could not calculate correlation metrics: {e}")
    
    # Class distribution comparison (if labels provided)
    if y_original is not None and y_synthetic is not None:
        try:
            y_orig = np.asarray(y_original)
            y_synth = np.asarray(y_synthetic)
            
            # Get class distributions
            unique_orig, counts_orig = np.unique(y_orig, return_counts=True)
            unique_synth, counts_synth = np.unique(y_synth, return_counts=True)
            
            # Normalize to proportions
            prop_orig = counts_orig / len(y_orig)
            prop_synth = counts_synth / len(y_synth)
            
            # Calculate KL divergence (simple version)
            # Add small epsilon to avoid log(0)
            eps = 1e-8
            prop_orig = prop_orig + eps
            prop_synth = prop_synth + eps
            
            # Ensure same classes in both
            all_classes = np.unique(np.concatenate([unique_orig, unique_synth]))
            
            prop_orig_full = np.zeros(len(all_classes)) + eps
            prop_synth_full = np.zeros(len(all_classes)) + eps
            
            for i, cls in enumerate(all_classes):
                if cls in unique_orig:
                    idx = np.where(unique_orig == cls)[0][0]
                    prop_orig_full[i] = prop_orig[idx]
                if cls in unique_synth:
                    idx = np.where(unique_synth == cls)[0][0]
                    prop_synth_full[i] = prop_synth[idx]
            
            # Normalize again
            prop_orig_full = prop_orig_full / np.sum(prop_orig_full)
            prop_synth_full = prop_synth_full / np.sum(prop_synth_full)
            
            # Calculate JS divergence (symmetric version of KL)
            m = 0.5 * (prop_orig_full + prop_synth_full)
            js_div = 0.5 * np.sum(prop_orig_full * np.log(prop_orig_full / m)) + \
                     0.5 * np.sum(prop_synth_full * np.log(prop_synth_full / m))
            
            metrics['js_divergence'] = js_div
            
        except Exception as e:
            warnings.warn(f"Could not calculate class distribution metrics: {e}")
    
    return metrics