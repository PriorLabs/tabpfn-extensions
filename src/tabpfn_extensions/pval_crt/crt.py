from __future__ import annotations

import numpy as np
import torch

from typing import Any, Dict, Optional
from sklearn.model_selection import train_test_split

try:
    from tabpfn import TabPFNRegressor, TabPFNClassifier
except ImportError as err:
    raise ImportError(
        "pval_crt requires the full TabPFN package and does not support "
        "the tabpfn-client backend."
    ) from err

from tabpfn.constants import ModelVersion

from .utils import is_categorical, logp_from_full_output, logp_from_proba

def tabpfn_crt(
    X: np.ndarray,
    y: np.ndarray,
    j: int,
    *,
    B: int = 200,
    alpha: float = 0.05,
    test_size: float = 0.2,
    seed: int = 0,
    device: Optional[str] = None,
    K: int = 100,
    max_unique_cat: int = 10,
) -> Dict[str, Any]:

    """
    Conditional Randomization Test (CRT) using TabPFN as a fixed predictive model.

    This function tests whether feature X[:, j] provides information about the target y
    beyond what is already contained in the remaining covariates X[:, -j], using the
    Conditional Randomization Test framework with TabPFN models for both:
        (1) the predictive model p(y | X), and
        (2) the conditional model p(X_j | X_-j).

    The test statistic is the mean log-predictive density of the evaluation data under
    the fitted TabPFN model. A null distribution is generated by repeatedly resampling
    X_j from its conditional distribution given X_-j and recomputing the statistic.

    Parameters
    ----------
    X : array-like of shape (n_samples, n_features)
        Feature matrix.

    y : array-like of shape (n_samples,)
        Target variable. If categorical (based on number of unique values), a
        TabPFNClassifier is used; otherwise, a TabPFNRegressor is used.

    j : int
        Index of the feature in X to test for conditional relevance.

    B : int, default=200
        Number of CRT resamples used to approximate the null distribution.

    alpha : float, default=0.05
        Significance level for the hypothesis test.

    test_size : float, default=0.2
        Fraction of data used for evaluation. The remaining data is used for fitting
        the TabPFN models.

    seed : int, default=0
        Random seed used for train/evaluation split and CRT resampling.

    device : {"cpu", "cuda"} or None, default=None
        Device used by TabPFN models. If None, uses CUDA if available, otherwise CPU.

    K : int, default=100
        Number of quantile levels used when approximating the conditional distribution
        of continuous X_j via TabPFN quantile regression.

    max_unique_cat : int, default=10
        Maximum number of unique values for a variable to be treated as categorical.
        Variables with at most this many unique values are modeled using
        TabPFNClassifier; otherwise TabPFNRegressor is used.

    Returns
    -------
    result : dict
        Dictionary containing:

        - "p_value" : float
            CRT p-value for testing conditional independence of X[:, j] and y.

        - "reject_null" : bool
            Whether the null hypothesis is rejected at level alpha.

        - "alpha" : float
            Significance level used for the test.

        - "T_obs" : float
            Observed test statistic (mean log-predictive density).

        - "T_null" : ndarray of shape (B,)
            Null distribution of the test statistic under conditional resampling.

        - "interpretation" : str
            Human-readable interpretation of the test result.

        - "y_is_categorical" : bool
            Whether the target variable was treated as categorical.

        - "xj_is_categorical" : bool
            Whether the tested feature was treated as categorical.

        - "B" : int
            Number of CRT resamples.

        - "K" : int or None
            Number of quantile levels used for continuous X_j (None if categorical).

        - "j" : int
            Index of the tested feature.

    Notes
    -----
    This implementation uses TabPFN models in "fit_with_cache" mode, treating them as
    fixed predictive models during the CRT procedure. No model refitting is performed
    across CRT resamples.

    The test is right-tailed, i.e., large predictive log-likelihood under the observed
    data relative to the null distribution indicates feature relevance.
    """

    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"

    rng = np.random.RandomState(seed)

    # ---------------------------
    # Split
    # ---------------------------
    X_tr, X_ev, y_tr, y_ev = train_test_split(
        np.asarray(X),
        np.asarray(y),
        test_size=test_size,
        random_state=seed,
        shuffle=True,
    )

    # ---------------------------
    # Model for y | X
    # ---------------------------
    y_is_cat = is_categorical(y_tr, max_unique_cat)
    ModelY = TabPFNClassifier if y_is_cat else TabPFNRegressor

    model_y = ModelY.create_default_for_version(
        ModelVersion.V2,
        device=device,
        fit_mode="fit_with_cache",
    )
    model_y.fit(X_tr, y_tr)

    if y_is_cat:
        probs_plus = model_y.predict_proba(X_ev)
        logp_plus = logp_from_proba(probs_plus, y_ev, model_y.classes_)
    else:
        full_plus = model_y.predict(X_ev, output_type="full")
        logp_plus = logp_from_full_output(full_plus, y_ev)

    # ---------------------------
    # Observed T_obs
    # ---------------------------
    T_obs = np.mean(logp_plus)

    # ---------------------------
    # Model for Xj | X_-j
    # ---------------------------
    Xm_tr = np.delete(X_tr, j, axis=1)
    Xm_ev = np.delete(X_ev, j, axis=1)
    xj_tr = X_tr[:, j]

    xj_is_cat = is_categorical(xj_tr, max_unique_cat)
    ModelXJ = TabPFNClassifier if xj_is_cat else TabPFNRegressor

    model_xj = ModelXJ.create_default_for_version(
        ModelVersion.V2,
        device=device,
        fit_mode="fit_with_cache",
    )
    model_xj.fit(Xm_tr, xj_tr)

    # ---------------------------
    # Precompute conditional sampler
    # ---------------------------
    if not xj_is_cat:
        q_grid = np.linspace(0, 1, K)
        Q = np.asarray(
            model_xj.predict(
                Xm_ev,
                output_type="quantiles",
                quantiles=q_grid,
            )
        )
        if Q.shape[0] != K:
            Q = Q.T  # ensure (K, n_ev)
    elif xj_is_cat:
        probs = model_xj.predict_proba(Xm_ev)
        cdf = np.cumsum(probs, axis=1)

        if not np.all(np.isfinite(probs)):
            i = np.argwhere(~np.isfinite(probs))[0][0]
            print("Non-finite probs at row", i, probs[i])
            raise ValueError("bad probs")
        row_sums = probs.sum(axis=1)
        if not np.allclose(row_sums, 1.0, atol=1e-6):
            i = np.argmax(np.abs(row_sums - 1.0))
            print("Bad prob sum at row", i, row_sums[i], probs[i])
            raise ValueError("probabilities not normalized")

    # ---------------------------
    # Null distribution
    # ---------------------------
    T_null = np.zeros(B)
    n_ev = X_ev.shape[0]
    X_ev_null = X_ev.copy()

    for b in range(B):
        if xj_is_cat:
            u = rng.rand(n_ev, 1)
            idx = (u <= cdf).argmax(axis=1)
            xj_null = model_xj.classes_[idx]
        else:
            idx = rng.randint(0, K, size=n_ev)
            xj_null = Q[idx, np.arange(n_ev)]

            bad = ~np.isfinite(xj_null)
            max_resample = 10
            n_try = 0

            while bad.any():
                if n_try >= max_resample:
                    raise RuntimeError(
                        f"CRT quantile sampling produced non-finite values after {max_resample} retries"
                    )

                # resample ONLY the bad positions
                idx_bad = rng.randint(0, K, size=bad.sum())
                xj_null[bad] = Q[idx_bad, np.where(bad)[0]]

                bad = ~np.isfinite(xj_null)
                n_try += 1

        X_ev_null[:, j] = np.asarray(xj_null)
        
        if y_is_cat:
            probs_null = model_y.predict_proba(X_ev_null)
            logp_null = logp_from_proba(probs_null, y_ev, model_y.classes_)
        else:
            full_null = model_y.predict(X_ev_null, output_type="full")
            logp_null = logp_from_full_output(full_null, y_ev)

        T_null[b] = np.mean(logp_null)
    # ---------------------------
    # p-value (right-tailed)
    # ---------------------------
    p_value = float((1 + np.sum(T_null >= T_obs)) / (B + 1))

    # ---------------------------
    # Human-readable interpretation
    # ---------------------------
    reject = p_value <= alpha

    if reject:
        relevance_stmt = (
            f"Result: REJECT H0 at α = {alpha:.2f}.\n"
            f"Interpretation: The variable X[{j}] provides information about the "
            f"target Y that is not explained by the remaining covariates."
        )
    else:
        relevance_stmt = (
            f"Result: FAIL TO REJECT H0 at α = {alpha:.2f}.\n"
            f"Interpretation: There is no evidence that X[{j}] provides additional "
            f"information about the target Y beyond the remaining covariates."
        )

    summary_stmt = (
        "\n=== Conditional Randomization Test (TabPFN) ===\n"
        f"p-value: {p_value:.4g}\n"
        f"{relevance_stmt}"
    )

    print(summary_stmt)

    return {
        "p_value": float(p_value),
        "reject_null": bool(reject),
        "alpha": alpha,
        "T_obs": T_obs,
        "T_null": T_null,
        "interpretation": relevance_stmt,
        "y_is_categorical": y_is_cat,
        "xj_is_categorical": xj_is_cat,
        "B": B,
        "K": K if not xj_is_cat else None,
        "j": int(j),
    }